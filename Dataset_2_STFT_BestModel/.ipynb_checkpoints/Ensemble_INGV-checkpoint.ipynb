{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble of MLP models trained on STFT dataset for better predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing essential libraries\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os \n",
    "import cv2\n",
    "import random \n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv1D,Conv2D,MaxPooling1D,Flatten,Dense,Dropout,BatchNormalization, GRU, LSTM, RNN\n",
    "from tensorflow.keras import regularizers as reg\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the test data for predictions\n",
    "pickle_in = open(\"X_test_T2.pickle\",\"rb\")\n",
    "X_test = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the models list \n",
    "models = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The models were trained on the STFT feature extracted dataset and were saved. The saved models are loaded \n",
    "#for Ensembling \n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "NN_model1 = load_model('model_F2T2.h5')\n",
    "NN_model2 = load_model('model_F3T2.h5')\n",
    "NN_model3 = load_model('model_F4T2.h5')\n",
    "NN_model4 = load_model('model_F5T2.h5')\n",
    "NN_model5 = load_model('model_F6T2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending the loaded models to the created list \n",
    "models.append(NN_model1)\n",
    "models.append(NN_model2)\n",
    "models.append(NN_model3)\n",
    "models.append(NN_model4)\n",
    "models.append(NN_model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the test dataset for the all the models appended\n",
    "yhats = [model.predict(X_test) for model in models]\n",
    "yhats_array = np.array(yhats)   #Converting the predictions to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4520\n"
     ]
    }
   ],
   "source": [
    "#Averaging the predicted values from each of the models to get the final predictions\n",
    "mean_predict=[]\n",
    "#count=0\n",
    "for i in range(yhats_array.shape[1]):\n",
    "    #count+=1\n",
    "    temp_predict=np.mean(yhats_array[:,i])   #Taking mean for the each model's predictions of every instance \n",
    "    mean_predict.append(temp_predict)\n",
    "#     if count > 4:\n",
    "#         break\n",
    "print(len(mean_predict))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsub =  pd.read_csv('C:/Datasets/predict-volcanic-eruptions-ingv-oe/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Submission file to kaggle\n",
    "submission = pd.DataFrame()\n",
    "submission['segment_id'] = testsub['segment_id']\n",
    "submission['time_to_eruption'] = mean_predict\n",
    "submission.to_csv('submission_MLPEns_121320_F5_repeat.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
