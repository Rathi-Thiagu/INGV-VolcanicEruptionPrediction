{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Data Generator and Iterator for the entire Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import importlib \n",
    "import csv\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv1D,Conv2D,MaxPooling1D,MaxPooling2D,Flatten,Dense,Dropout,BatchNormalization, GRU, LSTM, RNN\n",
    "from tensorflow.keras import regularizers as reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1136037770 1969647810 1895879680 ...  694853998 1886987043 1100632800]\n"
     ]
    }
   ],
   "source": [
    "volcano = pd.read_csv('E:/code/ml/datasets/predict-volcanic-eruptions-ingv-oe/train.csv')\n",
    "TRAINDIR = \"E:/code/ml/datasets/predict-volcanic-eruptions-ingv-oe/train\"\n",
    "TESTDIR = \"E:/code/ml/datasets/predict-volcanic-eruptions-ingv-oe/test\"\n",
    "print(volcano[\"segment_id\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  4431\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re            \n",
    "\n",
    "files = glob.glob(f\"E:/code/ml/datasets/predict-volcanic-eruptions-ingv-oe/train/*\")\n",
    "print(\"Total number of files: \", len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sensor_1  sensor_2  sensor_3  sensor_4  sensor_5  sensor_6  sensor_7  \\\n",
      "0          33.0     964.0    -226.0     536.0     143.0     344.0     -18.0   \n",
      "1          47.0     302.0    -257.0     494.0     171.0     358.0     -32.0   \n",
      "2          79.0    -431.0    -341.0     478.0     108.0     179.0     -21.0   \n",
      "3          98.0   -1069.0    -353.0     491.0     -62.0     123.0     -17.0   \n",
      "4         133.0   -1569.0    -397.0     499.0    -313.0     133.0     -26.0   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "59996     415.0   -1196.0    -161.0     -53.0     -30.0     344.0     181.0   \n",
      "59997     476.0   -1200.0     -88.0     -35.0     125.0     302.0     187.0   \n",
      "59998     530.0   -1261.0    -140.0     -19.0     277.0     354.0     204.0   \n",
      "59999     579.0   -1325.0     -14.0      16.0     407.0     329.0     227.0   \n",
      "60000     601.0   -1336.0     -20.0      55.0     484.0     292.0     244.0   \n",
      "\n",
      "       sensor_8  sensor_9  sensor_10  \n",
      "0          83.0    -100.0      -24.0  \n",
      "1         434.0    -127.0      -15.0  \n",
      "2         375.0    -181.0       33.0  \n",
      "3         537.0    -145.0      134.0  \n",
      "4         456.0    -190.0      300.0  \n",
      "...         ...       ...        ...  \n",
      "59996    -282.0      56.0     -777.0  \n",
      "59997    -120.0     -34.0     -670.0  \n",
      "59998    -413.0     -75.0     -555.0  \n",
      "59999    -242.0     -57.0     -423.0  \n",
      "60000    -376.0     -74.0     -310.0  \n",
      "\n",
      "[60001 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "train_set = pd.DataFrame()\n",
    "train_set['segment_id'] = volcano.segment_id\n",
    "train_set = train_set.set_index('segment_id')\n",
    "j = 0\n",
    "for seg in volcano.segment_id:\n",
    "    signals = pd.read_csv(f'E:/code/ml/datasets/predict-volcanic-eruptions-ingv-oe/train/{seg}.csv').replace([np.inf, -np.inf, np.nan], 0)\n",
    "    j = j+1\n",
    "    if j > 1: break\n",
    "print(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signals.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(signals)\n",
    "signals_normalized = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0         1         2         3         4         5         6  \\\n",
      "0      0.485007  0.557795  0.489390  0.660797  0.579398  0.567749  0.589384   \n",
      "1      0.488339  0.487204  0.481268  0.650526  0.586663  0.571669  0.587047   \n",
      "2      0.495954  0.409042  0.459261  0.646613  0.570317  0.521557  0.588883   \n",
      "3      0.500476  0.341011  0.456117  0.649792  0.526207  0.505879  0.589551   \n",
      "4      0.508805  0.287695  0.444590  0.651749  0.461079  0.508679  0.588049   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "59996  0.575916  0.327469  0.506419  0.516752  0.534510  0.567749  0.622601   \n",
      "59997  0.590433  0.327042  0.525544  0.521154  0.574728  0.555991  0.623602   \n",
      "59998  0.603284  0.320537  0.511920  0.525067  0.614167  0.570549  0.626440   \n",
      "59999  0.614945  0.313713  0.544931  0.533627  0.647898  0.563550  0.630279   \n",
      "60000  0.620181  0.312540  0.543359  0.543165  0.667878  0.553191  0.633116   \n",
      "\n",
      "              7         8         9  \n",
      "0      0.563596  0.523522  0.460848  \n",
      "1      0.673559  0.517902  0.461756  \n",
      "2      0.655075  0.506661  0.466599  \n",
      "3      0.705827  0.514155  0.476791  \n",
      "4      0.680451  0.504788  0.493542  \n",
      "...         ...       ...       ...  \n",
      "59996  0.449248  0.555995  0.384864  \n",
      "59997  0.500000  0.537261  0.395661  \n",
      "59998  0.408208  0.528726  0.407265  \n",
      "59999  0.461779  0.532473  0.420585  \n",
      "60000  0.419799  0.528934  0.431988  \n",
      "\n",
      "[60001 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "print(signals_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_check = f\"E:/code/ml/datasets/predict-volcanic-eruptions-ingv-oe/train/13787554.csv\"\n",
    "nb = pd.read_csv(file_check).replace([np.inf, -np.inf, np.nan], 0)\n",
    "nb.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4431\n"
     ]
    }
   ],
   "source": [
    "def data_normalizing(signals):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(signals)\n",
    "    signals_normalized = pd.DataFrame(x_scaled)\n",
    "    return(signals_normalized)\n",
    "\n",
    "def ip_data(filename):\n",
    "    filename = sanitize(filename)\n",
    "    notebook = pd.read_csv(filename).replace([np.inf, -np.inf, np.nan], 0)\n",
    "    notebook_normalized = data_normalizing(notebook)\n",
    "    return notebook_normalized\n",
    "\n",
    "def sanitize(string):\n",
    "    try:\n",
    "        string = string.decode()\n",
    "    except (UnicodeDecodeError, AttributeError):\n",
    "        pass\n",
    "    return string\n",
    "\n",
    "def get_file_name(file_path):\n",
    "    file_path = sanitize(file_path)\n",
    "    return file_path.split(\"\\\\\")[1].split(\".\")[0]\n",
    "\n",
    "X_Id = volcano['segment_id']\n",
    "X_Erupt = volcano['time_to_eruption']\n",
    "\n",
    "X_dict = dict(zip(X_Id, X_Erupt))\n",
    "\n",
    "training = pd.DataFrame()\n",
    "training['segment_id'] = volcano.segment_id\n",
    "training = training.set_index('segment_id')\n",
    "\n",
    "def iterator_train(files, batch_size):\n",
    "    for i in range(len(files)):\n",
    "        end = i + batch_size\n",
    "        data = []\n",
    "        labels = []\n",
    "        for j in range(i, end):\n",
    "            current_file = files[j]\n",
    "            pattern = get_file_name(current_file)\n",
    "            labels = X_dict[int(pattern)]\n",
    "            labels = np.asarray(labels).reshape(-1)\n",
    "            \n",
    "            data = ip_data(files[j])    \n",
    "            data = np.asarray(data).reshape(-1,60001,10,1)            \n",
    "\n",
    "#             print(type(labels))\n",
    "            \n",
    "            yield data, labels\n",
    "\n",
    "iterator_train(files, 5)\n",
    "print(len(files))\n",
    "# print(type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_per_epoch =  2216\n",
      "Epoch 1/5\n",
      "2216/2216 [==============================] - 303s 137ms/step - loss: 16470116.0000 - mean_absolute_error: 16470092.0000\n",
      "Epoch 2/5\n",
      "2216/2216 [==============================] - 293s 132ms/step - loss: 10819907.0000 - mean_absolute_error: 10819866.0000\n",
      "Epoch 3/5\n",
      "2216/2216 [==============================] - 289s 130ms/step - loss: 11400713.0000 - mean_absolute_error: 11400648.0000\n",
      "Epoch 4/5\n",
      "2216/2216 [==============================] - 286s 129ms/step - loss: 10630703.0000 - mean_absolute_error: 10630626.0000\n",
      "Epoch 5/5\n",
      "2216/2216 [==============================] - 286s 129ms/step - loss: 10539270.0000 - mean_absolute_error: 10539185.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dbc3248b20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 5\n",
    "\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(64, 3, input_shape = (60001,10,1), activation = 'relu',\n",
    "                     kernel_initializer='normal',kernel_regularizer=reg.l2(0.05)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(MaxPooling2D(pool_size = 2))\n",
    "cnn_model.add(Conv2D(128, 3, activation = 'relu',kernel_initializer='normal',kernel_regularizer=reg.l2(0.05)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(MaxPooling2D(pool_size = 2))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(1, activation = 'linear'))\n",
    "cnn_model.compile(optimizer = 'adam', loss = 'mean_absolute_error', metrics = ['mean_absolute_error'])\n",
    "\n",
    "# cnn_model.summary()\n",
    "\n",
    "steps_per_epoch = np.int(np.ceil(len(files)/2))\n",
    "print(\"steps_per_epoch = \", steps_per_epoch)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(iterator_train, args = [files, BATCH_SIZE],\n",
    "                                               output_shapes = ((None,60001,10,1),(None,)),\n",
    "                                              output_types = (tf.float32, tf.float32))\n",
    "cnn_model.fit(train_dataset, steps_per_epoch = steps_per_epoch, epochs=5)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = pd.read_csv('E:/code/ml/datasets/predict-volcanic-eruptions-ingv-oe/sample_submission.csv')\n",
    "\n",
    "def data_normalizing(signals):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.transform(signals)\n",
    "    signals_normalized = pd.DataFrame(x_scaled)\n",
    "    return(signals_normalized)\n",
    "\n",
    "def ip_data(filename):\n",
    "    filename = sanitize(filename)\n",
    "    notebook = pd.read_csv(filename).replace([np.inf, -np.inf, np.nan], 0)\n",
    "    notebook_normalized = data_normalizing(notebook)\n",
    "    return notebook_normalized\n",
    "\n",
    "def sanitize(string):\n",
    "    try:\n",
    "        string = string.decode()\n",
    "    except (UnicodeDecodeError, AttributeError):\n",
    "        pass\n",
    "    return string\n",
    "\n",
    "def iterator_test(files, batch_size):\n",
    "    for i in range(len(files)):\n",
    "        end = i + batch_size\n",
    "        data = []\n",
    "        labels = []\n",
    "        for j in range(i, end):\n",
    "            current_file = files[j]\n",
    "            \n",
    "            data = ip_data(files[j])    \n",
    "            data = np.asarray(data).reshape(-1,60001,10,1)           \n",
    "            \n",
    "            yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4520\n"
     ]
    }
   ],
   "source": [
    "files_test = glob.glob(f\"E:/code/ml/datasets/predict-volcanic-eruptions-ingv-oe/test/*\")\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "steps_per_epocht = np.int(np.ceil(len(files_test)))\n",
    "\n",
    "prediction_dataset = tf.data.Dataset.from_generator(iterator_test, args = [files_test, BATCH_SIZE],\n",
    "                               output_shapes=(None,60001,10,1), output_types=(tf.float32))\n",
    "\n",
    "predictions = cnn_model.predict(prediction_dataset, steps = steps_per_epocht)\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0367773e+08]\n",
      " [1.0049795e+08]\n",
      " [1.8562946e+07]\n",
      " ...\n",
      " [2.4311878e+07]\n",
      " [1.3868736e+07]\n",
      " [1.1725846e+07]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(f'E:/code/ml/datasets/predict-volcanic-eruptions-ingv-oe/sample_submission.csv')\n",
    "\n",
    "submision = pd.DataFrame()\n",
    "submision['segment_id'] = test_set.segment_id\n",
    "submision['time_to_eruption'] = predictions\n",
    "submision.to_csv('submit_CNN_Bby2.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
